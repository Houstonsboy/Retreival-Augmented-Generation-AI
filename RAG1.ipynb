{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOt59zm3650Srk4ewcmx0YE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Houstonsboy/Retreival-Augmented-Generation-AI/blob/master/RAG1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mSoaB2keX8MU"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF to read PDFs\n",
        "import os\n",
        "from google.colab import drive\n",
        "import re\n",
        "import tiktoken\n",
        "import time\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import List, Dict\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langdetect import detect, LangDetectException"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyMuPDF\n"
      ],
      "metadata": {
        "id": "S0pT_xOjRlEQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3c65455",
        "outputId": "e898d70f-80be-4bd7-d83c-efec4752c658"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0437d873",
        "outputId": "81dd0326-1371-4046-c575-e09be3488bc1"
      },
      "source": [
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebd89e72",
        "outputId": "915a594e-ec8b-462a-9a7e-c0fe621d96cc"
      },
      "source": [
        "import os\n",
        "\n",
        "drive_path = '/content/drive/My Drive'\n",
        "if os.path.exists(drive_path):\n",
        "  files = os.listdir(drive_path)\n",
        "  print(files)\n",
        "else:\n",
        "  print(f\"The directory {drive_path} does not exist. Please ensure Google Drive is mounted correctly.\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IMG-20230723-WA0010(1).jpg', 'Classroom', 'Node.java', 'Bubblesort (1).java', 'Bubblesort.java', 'EGYPT CIVILIZATION (1).rtf.gdoc', 'EGYPT CIVILIZATION.rtf.gdoc', 'netmesh', '167998-NandAstableMultivibrator', '3 - Adv . DB - Transaction Management-1.gdoc', 'Untitled0.ipynb', 'Colab Notebooks', 'melb_data.csv', 'melbv1.csv', 'Managing User Password.gdoc', 'Untitled document (2).gdoc', 'KRISTEIN GICHUHI MWAURA.gdoc', 'Managing file system permission.gdoc', '167998-SALabPART 2.gdoc', 'Controlling access to files.gdoc', 'LAB2 -SECRET ENCRYPTION LAB.gdoc', 'Untitled document (1).gdoc', 'Untitled document.gdoc', 'Bobs_superheroes.gdoc', 'Bobs_superheroes.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_document(text: str, min_tokens=500, max_tokens=800, overlap_percent=20, title=\"Document\"):\n",
        "    \"\"\"\n",
        "    Chunk document text into semantic, token-based chunks.\n",
        "\n",
        "    Args:\n",
        "        text: Extracted text from PDF\n",
        "        min_tokens: Minimum tokens per chunk (default: 500)\n",
        "        max_tokens: Maximum tokens per chunk (default: 800)\n",
        "        overlap_percent: Overlap percentage between chunks (default: 20)\n",
        "        title: Document title for metadata (default: \"Document\")\n",
        "\n",
        "    Returns:\n",
        "        List of chunk dictionaries with metadata\n",
        "    \"\"\"\n",
        "    # Initialize tokenizer\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    except:\n",
        "        import subprocess\n",
        "        subprocess.check_call(['pip', 'install', '-q', 'tiktoken'])\n",
        "        import tiktoken\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def count_tokens(txt):\n",
        "        return len(encoding.encode(txt))\n",
        "\n",
        "    def split_sentences(txt):\n",
        "        \"\"\"Split text into sentences preserving meaning.\"\"\"\n",
        "        txt = re.sub(r'(\\w)\\.(\\s+[A-Z])', r'\\1.<SPLIT>\\2', txt)\n",
        "        txt = re.sub(r'(Mr|Mrs|Ms|Dr|Prof|Sr|Jr|Inc|Ltd|Co)\\.', r'\\1<DOT>', txt)\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', txt)\n",
        "        sentences = [s.replace('<DOT>', '.').replace('<SPLIT>', '') for s in sentences]\n",
        "        return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Calculate overlap\n",
        "    overlap_tokens = int(max_tokens * overlap_percent / 100)\n",
        "\n",
        "    # Split into sentences\n",
        "    sentences = split_sentences(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_tokens = 0\n",
        "    chunk_num = 1\n",
        "    overlap_buffer = []\n",
        "    overlap_buffer_tokens = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = count_tokens(sentence)\n",
        "\n",
        "        # Check if adding sentence exceeds max_tokens\n",
        "        if current_tokens + sentence_tokens > max_tokens and current_chunk:\n",
        "            # Save current chunk if meets minimum\n",
        "            if current_tokens >= min_tokens:\n",
        "                chunk_text = ' '.join(current_chunk)\n",
        "                chunks.append({\n",
        "                    'chunk_number': chunk_num,\n",
        "                    'title': title,\n",
        "                    'text': chunk_text,\n",
        "                    'token_count': current_tokens,\n",
        "                    'sentence_count': len(current_chunk),\n",
        "                    'char_count': len(chunk_text)\n",
        "                })\n",
        "                chunk_num += 1\n",
        "\n",
        "                # Build overlap buffer\n",
        "                overlap_buffer = []\n",
        "                overlap_buffer_tokens = 0\n",
        "                for sent in reversed(current_chunk):\n",
        "                    sent_tokens = count_tokens(sent)\n",
        "                    if overlap_buffer_tokens + sent_tokens <= overlap_tokens:\n",
        "                        overlap_buffer.insert(0, sent)\n",
        "                        overlap_buffer_tokens += sent_tokens\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # Start new chunk with overlap\n",
        "                current_chunk = overlap_buffer.copy()\n",
        "                current_tokens = overlap_buffer_tokens\n",
        "\n",
        "        # Add sentence to current chunk\n",
        "        current_chunk.append(sentence)\n",
        "        current_tokens += sentence_tokens\n",
        "\n",
        "    # Add final chunk\n",
        "    if current_chunk and current_tokens >= min_tokens:\n",
        "        chunk_text = ' '.join(current_chunk)\n",
        "        chunks.append({\n",
        "            'chunk_number': chunk_num,\n",
        "            'title': title,\n",
        "            'text': chunk_text,\n",
        "            'token_count': current_tokens,\n",
        "            'sentence_count': len(current_chunk),\n",
        "            'char_count': len(chunk_text)\n",
        "        })\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def view_chunk(chunks: List[Dict], n: int):\n",
        "    \"\"\"\n",
        "    Display a specific chunk with its metadata.\n",
        "\n",
        "    Args:\n",
        "        chunks: List of chunks returned by chunk_document()\n",
        "        n: Chunk number to display (1-indexed)\n",
        "    \"\"\"\n",
        "    if n < 1 or n > len(chunks):\n",
        "        print(f\"‚ùå Error: Chunk {n} does not exist. Valid range: 1-{len(chunks)}\")\n",
        "        return\n",
        "\n",
        "    chunk = chunks[n - 1]\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üìÑ CHUNK {chunk['chunk_number']} | {chunk['title']}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"üìä Tokens: {chunk['token_count']} | Sentences: {chunk['sentence_count']} | Characters: {chunk['char_count']}\")\n",
        "    print(f\"{'-'*80}\")\n",
        "    print(chunk['text'])\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def print_summary(chunks: List[Dict]):\n",
        "    \"\"\"Print summary of all chunks.\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üìö CHUNKING SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total chunks: {len(chunks)}\")\n",
        "    if chunks:\n",
        "        print(f\"Token range: {chunks[0]['token_count']}-{max(c['token_count'] for c in chunks)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for chunk in chunks:\n",
        "        preview = chunk['text'][:100] + \"...\" if len(chunk['text']) > 100 else chunk['text']\n",
        "        print(f\"Chunk {chunk['chunk_number']}: {chunk['token_count']} tokens | {preview}\")\n"
      ],
      "metadata": {
        "id": "Nf6asRx4X0Lq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WuCzlGORYO9U"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def install_dependencies():\n",
        "#     \"\"\"Install required packages for embeddings and vector database.\"\"\"\n",
        "#     print(\"üì¶ Installing dependencies...\")\n",
        "#     import subprocess\n",
        "\n",
        "#     packages = [\n",
        "#         'chromadb',\n",
        "#         'sentence-transformers',\n",
        "#         'langdetect'\n",
        "#     ]\n",
        "\n",
        "#     for package in packages:\n",
        "#         print(f\"   Installing {package}...\")\n",
        "#         subprocess.check_call(['pip', 'install', '-q', package])\n",
        "\n",
        "#     print(\"‚úÖ All dependencies installed!\\n\")\n",
        "\n",
        "# install_dependencies()\n"
      ],
      "metadata": {
        "id": "Ue00xQSkkT_o"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_embedding_model(model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"\n",
        "    Initialize the embedding model.\n",
        "\n",
        "    Available models (sorted by quality/size):\n",
        "    - 'all-MiniLM-L6-v2': Fast, 384 dims (Recommended for speed)\n",
        "    - 'all-mpnet-base-v2': Best quality, 768 dims (Recommended for accuracy)\n",
        "    - 'multi-qa-mpnet-base-dot-v1': Great for Q&A tasks, 768 dims\n",
        "    - 'paraphrase-multilingual-MiniLM-L12-v2': Multilingual, 384 dims\n",
        "\n",
        "    Args:\n",
        "        model_name: Name of the sentence-transformer model\n",
        "\n",
        "    Returns:\n",
        "        Loaded embedding model\n",
        "    \"\"\"\n",
        "    print(f\"üß† Initializing embedding model: {model_name}\")\n",
        "    print(\"   (First run will download the model...)\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    print(f\"‚úÖ Model loaded in {time.time() - start_time:.2f}s\")\n",
        "    print(f\"   Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "    print()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: INITIALIZE CHROMADB (FREE VECTOR DATABASE)\n",
        "# =============================================================================\n",
        "\n",
        "def initialize_vector_db(collection_name='document_chunks', persist_directory='./chroma_db'):\n",
        "    \"\"\"\n",
        "    Initialize ChromaDB - a free, robust vector database.\n",
        "\n",
        "    Args:\n",
        "        collection_name: Name for your collection\n",
        "        persist_directory: Directory to persist the database\n",
        "\n",
        "    Returns:\n",
        "        ChromaDB collection object\n",
        "    \"\"\"\n",
        "    print(f\"üíæ Initializing ChromaDB...\")\n",
        "    print(f\"   Collection: {collection_name}\")\n",
        "    print(f\"   Persist directory: {persist_directory}\")\n",
        "\n",
        "    # Initialize ChromaDB client with persistence\n",
        "    client = chromadb.PersistentClient(path=persist_directory)\n",
        "\n",
        "    # Create or get collection\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=collection_name,\n",
        "        metadata={\"description\": \"Document chunks with embeddings\"}\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ ChromaDB initialized!\")\n",
        "    print(f\"   Existing documents in collection: {collection.count()}\")\n",
        "    print()\n",
        "\n",
        "    return collection\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: DETECT LANGUAGE\n",
        "# =============================================================================\n",
        "\n",
        "def detect_language(text: str) -> str:\n",
        "    \"\"\"Detect language of text.\"\"\"\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: EMBED CHUNKS WITH BATCH PROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def embed_chunks(chunks: List[Dict],\n",
        "                 embedding_model,\n",
        "                 file_id: str = \"default_file\",\n",
        "                 batch_size: int = 128,\n",
        "                 normalize: bool = True):\n",
        "    \"\"\"\n",
        "    Generate embeddings for chunks with batch processing and L2 normalization.\n",
        "\n",
        "    Args:\n",
        "        chunks: List of chunk dictionaries from chunk_document()\n",
        "        embedding_model: Loaded SentenceTransformer model\n",
        "        file_id: Unique identifier for the source file\n",
        "        batch_size: Number of chunks to process per batch (default: 128)\n",
        "        normalize: Whether to L2-normalize embeddings for cosine similarity\n",
        "\n",
        "    Returns:\n",
        "        List of chunks with embeddings and metadata\n",
        "    \"\"\"\n",
        "    print(f\"üîÆ Generating embeddings for {len(chunks)} chunks...\")\n",
        "    print(f\"   Batch size: {batch_size}\")\n",
        "    print(f\"   L2 normalization: {normalize}\")\n",
        "    print()\n",
        "\n",
        "    embedded_chunks = []\n",
        "    total_batches = (len(chunks) + batch_size - 1) // batch_size\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i:i + batch_size]\n",
        "        batch_num = i // batch_size + 1\n",
        "\n",
        "        print(f\"   Processing batch {batch_num}/{total_batches} ({len(batch)} chunks)...\")\n",
        "\n",
        "        # Extract texts for embedding\n",
        "        texts = [chunk['text'] for chunk in batch]\n",
        "\n",
        "        # Generate embeddings\n",
        "        batch_embeddings = embedding_model.encode(\n",
        "            texts,\n",
        "            normalize_embeddings=normalize,\n",
        "            show_progress_bar=False\n",
        "        )\n",
        "\n",
        "        # Add embeddings and metadata to chunks\n",
        "        for j, chunk in enumerate(batch):\n",
        "            embedding = batch_embeddings[j]\n",
        "\n",
        "            # Detect language\n",
        "            language = detect_language(chunk['text'])\n",
        "\n",
        "            # Create enriched chunk with metadata\n",
        "            enriched_chunk = {\n",
        "                **chunk,  # Original chunk data\n",
        "                'embedding': embedding.tolist(),  # Convert numpy to list\n",
        "                'file_id': file_id,\n",
        "                'page_no': None,  # Will be updated if page info available\n",
        "                'heading': chunk.get('title', 'Unknown'),\n",
        "                'chunk_text': chunk['text'],\n",
        "                'embedding_model': getattr(embedding_model, 'model_name_or_path', 'all-MiniLM-L6-v2'),\n",
        "                'ingest_time': datetime.now().isoformat(),\n",
        "                'language': language,\n",
        "                'embedding_dim': len(embedding)\n",
        "            }\n",
        "\n",
        "            embedded_chunks.append(enriched_chunk)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"\\n‚úÖ Embeddings generated in {elapsed_time:.2f}s\")\n",
        "    print(f\"   Average: {elapsed_time/len(chunks):.3f}s per chunk\")\n",
        "    print(f\"   Throughput: {len(chunks)/elapsed_time:.1f} chunks/second\")\n",
        "    print()\n",
        "\n",
        "    return embedded_chunks\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: STORE IN VECTOR DATABASE\n",
        "# =============================================================================\n",
        "\n",
        "def store_in_vector_db(collection, embedded_chunks: List[Dict]):\n",
        "    \"\"\"\n",
        "    Store embedded chunks in ChromaDB with all metadata.\n",
        "\n",
        "    Args:\n",
        "        collection: ChromaDB collection\n",
        "        embedded_chunks: Chunks with embeddings from embed_chunks()\n",
        "    \"\"\"\n",
        "    print(f\"üíæ Storing {len(embedded_chunks)} chunks in vector database...\")\n",
        "\n",
        "    # Prepare data for ChromaDB\n",
        "    ids = []\n",
        "    embeddings = []\n",
        "    documents = []\n",
        "    metadatas = []\n",
        "\n",
        "    for chunk in embedded_chunks:\n",
        "       chunk_id = f\"{chunk['file_id']}_chunk_{chunk['chunk_number']}\"\n",
        "       ids.append(chunk_id)\n",
        "       embeddings.append(chunk['embedding'])\n",
        "       documents.append(chunk['chunk_text'])\n",
        "\n",
        "       metadata = {\n",
        "        'chunk_number': chunk.get('chunk_number', -1),\n",
        "        'title': chunk.get('title', 'unknown'),\n",
        "        'file_id': chunk.get('file_id', 'unknown'),\n",
        "        'page_no': chunk.get('page_no', 'unknown'),\n",
        "        'heading': chunk.get('heading', 'unknown'),\n",
        "        'token_count': chunk.get('token_count', 0),\n",
        "        'sentence_count': chunk.get('sentence_count', 0),\n",
        "        'char_count': chunk.get('char_count', 0),\n",
        "        'embedding_model': chunk.get('embedding_model', 'unknown'),\n",
        "        'ingest_time': chunk.get('ingest_time', 'unknown'),\n",
        "        'language': chunk.get('language', 'unknown'),\n",
        "        'embedding_dim': chunk.get('embedding_dim', 0)\n",
        "    }\n",
        "\n",
        "    # Clean out None values\n",
        "       clean_metadata = {k: (v if v is not None else \"unknown\") for k, v in metadata.items()}\n",
        "       metadatas.append(clean_metadata)\n",
        "\n",
        "\n",
        "    # Add to collection\n",
        "    start_time = time.time()\n",
        "    collection.add(\n",
        "        ids=ids,\n",
        "        embeddings=embeddings,\n",
        "        documents=documents,\n",
        "        metadatas=metadatas\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"‚úÖ Stored in {elapsed_time:.2f}s\")\n",
        "    print(f\"   Total documents in collection: {collection.count()}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: QUERY VECTOR DATABASE\n",
        "# =============================================================================\n",
        "\n",
        "def query_vector_db(collection,\n",
        "                    embedding_model,\n",
        "                    query_text: str,\n",
        "                    n_results: int = 5,\n",
        "                    filter_metadata: Dict = None):\n",
        "    \"\"\"\n",
        "    Query the vector database for similar chunks.\n",
        "\n",
        "    Args:\n",
        "        collection: ChromaDB collection\n",
        "        embedding_model: Loaded SentenceTransformer model\n",
        "        query_text: Text to search for\n",
        "        n_results: Number of results to return\n",
        "        filter_metadata: Optional metadata filters (e.g., {'file_id': 'doc1'})\n",
        "\n",
        "    Returns:\n",
        "        Query results with documents and metadata\n",
        "    \"\"\"\n",
        "    print(f\"üîç Querying: '{query_text}'\")\n",
        "    print(f\"   Returning top {n_results} results\")\n",
        "\n",
        "    # Generate query embedding\n",
        "    query_embedding = embedding_model.encode(\n",
        "        [query_text],\n",
        "        normalize_embeddings=True\n",
        "    )[0].tolist()\n",
        "\n",
        "    # Query the collection\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=n_results,\n",
        "        where=filter_metadata\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Found {len(results['documents'][0])} results\\n\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_query_results(results):\n",
        "    \"\"\"Print query results in a readable format.\"\"\"\n",
        "    documents = results['documents'][0]\n",
        "    metadatas = results['metadatas'][0]\n",
        "    distances = results['distances'][0]\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"SEARCH RESULTS\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances), 1):\n",
        "        similarity = 1 - dist  # Convert distance to similarity\n",
        "        print(f\"Result {i} | Similarity: {similarity:.4f}\")\n",
        "        print(f\"   File: {meta['file_id']} | Chunk: {meta['chunk_number']}\")\n",
        "        print(f\"   Title: {meta['title']}\")\n",
        "        print(f\"   Tokens: {meta['token_count']} | Language: {meta['language']}\")\n",
        "        print(f\"   Ingested: {meta['ingest_time']}\")\n",
        "        print(f\"\\n   Text preview: {doc[:200]}...\")\n",
        "        print(f\"{'-'*80}\\n\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: VERIFICATION FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def verify_embeddings(embedded_chunks: List[Dict], sample_size: int = 3):\n",
        "    \"\"\"Verify embeddings were created correctly.\"\"\"\n",
        "    print(f\"üîç EMBEDDING VERIFICATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total chunks embedded: {len(embedded_chunks)}\")\n",
        "\n",
        "    if embedded_chunks:\n",
        "        sample = embedded_chunks[:sample_size]\n",
        "\n",
        "        for i, chunk in enumerate(sample, 1):\n",
        "            print(f\"\\nChunk {i}:\")\n",
        "            print(f\"   Chunk Number: {chunk['chunk_number']}\")\n",
        "            print(f\"   Title: {chunk['title']}\")\n",
        "            print(f\"   File ID: {chunk['file_id']}\")\n",
        "            print(f\"   Language: {chunk['language']}\")\n",
        "            print(f\"   Embedding Model: {chunk['embedding_model']}\")\n",
        "            print(f\"   Embedding Dimension: {chunk['embedding_dim']}\")\n",
        "            print(f\"   Token Count: {chunk['token_count']}\")\n",
        "            print(f\"   Ingest Time: {chunk['ingest_time']}\")\n",
        "            print(f\"   Text Preview: {chunk['chunk_text'][:100]}...\")\n",
        "\n",
        "            # Check embedding\n",
        "            embedding = np.array(chunk['embedding'])\n",
        "            print(f\"   Embedding Stats:\")\n",
        "            print(f\"      - Shape: {embedding.shape}\")\n",
        "            print(f\"      - Mean: {embedding.mean():.4f}\")\n",
        "            print(f\"      - Std: {embedding.std():.4f}\")\n",
        "            print(f\"      - L2 Norm: {np.linalg.norm(embedding):.4f}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "\n",
        "def verify_vector_db(collection):\n",
        "    \"\"\"Verify vector database contents.\"\"\"\n",
        "    print(f\"üîç VECTOR DATABASE VERIFICATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Collection Name: {collection.name}\")\n",
        "    print(f\"Total Documents: {collection.count()}\")\n",
        "\n",
        "    if collection.count() > 0:\n",
        "        # Peek at first few documents\n",
        "        sample = collection.peek(limit=3)\n",
        "\n",
        "        print(f\"\\nSample Documents:\")\n",
        "        for i, (id, meta) in enumerate(zip(sample['ids'], sample['metadatas']), 1):\n",
        "            print(f\"\\n   Document {i}:\")\n",
        "            print(f\"      ID: {id}\")\n",
        "            print(f\"      Chunk: {meta['chunk_number']}\")\n",
        "            print(f\"      Title: {meta['title']}\")\n",
        "            print(f\"      File: {meta['file_id']}\")\n",
        "            print(f\"      Language: {meta['language']}\")\n",
        "            print(f\"      Tokens: {meta['token_count']}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPLETE WORKFLOW EXAMPLE\n",
        "# =============================================================================\n",
        "\n",
        "def complete_embedding_workflow(chunks, file_id=\"bobs_superheroes\"):\n",
        "    \"\"\"\n",
        "    Complete workflow: Initialize ‚Üí Embed ‚Üí Store ‚Üí Query\n",
        "\n",
        "    Args:\n",
        "        chunks: Chunks from chunk_document() function\n",
        "        file_id: Unique identifier for the source file\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"# COMPLETE EMBEDDING WORKFLOW\")\n",
        "    print(f\"{'#'*80}\\n\")\n",
        "\n",
        "    # Step 1: Initialize embedding model\n",
        "    print(\"STEP 1: Initialize Embedding Model\")\n",
        "    print(\"-\" * 80)\n",
        "    embedding_model = initialize_embedding_model('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Step 2: Initialize vector database\n",
        "    print(\"STEP 2: Initialize Vector Database\")\n",
        "    print(\"-\" * 80)\n",
        "    collection = initialize_vector_db(collection_name='superhero_docs')\n",
        "\n",
        "    # Step 3: Generate embeddings\n",
        "    print(\"STEP 3: Generate Embeddings\")\n",
        "    print(\"-\" * 80)\n",
        "    embedded_chunks = embed_chunks(\n",
        "        chunks=chunks,\n",
        "        embedding_model=embedding_model,\n",
        "        file_id=file_id,\n",
        "        batch_size=128,\n",
        "        normalize=True\n",
        "    )\n",
        "\n",
        "    # Step 4: Verify embeddings\n",
        "    print(\"STEP 4: Verify Embeddings\")\n",
        "    print(\"-\" * 80)\n",
        "    verify_embeddings(embedded_chunks)\n",
        "\n",
        "    # Step 5: Store in vector database\n",
        "    print(\"STEP 5: Store in Vector Database\")\n",
        "    print(\"-\" * 80)\n",
        "    store_in_vector_db(collection, embedded_chunks)\n",
        "\n",
        "    # Step 6: Verify database\n",
        "    print(\"STEP 6: Verify Database Storage\")\n",
        "    print(\"-\" * 80)\n",
        "    verify_vector_db(collection)\n",
        "\n",
        "    # Step 7: Test query\n",
        "    print(\"STEP 7: Test Query\")\n",
        "    print(\"-\" * 80)\n",
        "    results = query_vector_db(\n",
        "        collection=collection,\n",
        "        embedding_model=embedding_model,\n",
        "        query_text=\"Who is James\",\n",
        "        n_results=5\n",
        "    )\n",
        "    print_query_results(results)\n",
        "\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"# WORKFLOW COMPLETE!\")\n",
        "    print(f\"{'#'*80}\\n\")\n",
        "\n",
        "    return embedding_model, collection\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# USAGE WITH YOUR CHUNKS\n",
        "# =============================================================================\n",
        "\n",
        "\"\"\"\n",
        "# Assuming you have chunks from the previous chunking code:\n",
        "# chunks = chunk_document(text, title=\"Bobs Superheroes\")\n",
        "\n",
        "# Run the complete workflow:\n",
        "embedding_model, collection = complete_embedding_workflow(\n",
        "    chunks=chunks,\n",
        "    file_id=\"bobs_superheroes_2024\"\n",
        ")\n",
        "\n",
        "# Now you can query anytime:\n",
        "results = query_vector_db(\n",
        "    collection=collection,\n",
        "    embedding_model=embedding_model,\n",
        "    query_text=\"your search query here\",\n",
        "    n_results=5\n",
        ")\n",
        "print_query_results(results)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "7jcTWvrPlu5I",
        "outputId": "d6e42372-7a09-42c2-9bc9-15e7f7af8460"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Assuming you have chunks from the previous chunking code:\\n# chunks = chunk_document(text, title=\"Bobs Superheroes\")\\n\\n# Run the complete workflow:\\nembedding_model, collection = complete_embedding_workflow(\\n    chunks=chunks,\\n    file_id=\"bobs_superheroes_2024\"\\n)\\n\\n# Now you can query anytime:\\nresults = query_vector_db(\\n    collection=collection,\\n    embedding_model=embedding_model,\\n    query_text=\"your search query here\",\\n    n_results=5\\n)\\nprint_query_results(results)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = '/content/drive/My Drive/Bobs_superheroes.pdf'\n",
        "doc = fitz.open(pdf_path)\n",
        "text = ''\n",
        "for page in doc:\n",
        "    text += page.get_text()\n",
        "doc.close()\n",
        "\n",
        "# Create chunks (from previous code)\n",
        "chunks = chunk_document(text, title=\"Bobs Superheroes\")\n",
        "\n",
        "# Run complete embedding workflow with verification at each step\n",
        "\n",
        "# print_summary(chunks)   # Optional ‚Äî to see overview\n",
        "view_chunk(chunks, 8)   # üëà View chunk 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxeOJDZISAqH",
        "outputId": "e272a35d-25fd-4914-a380-d0ca9515ac22"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üìÑ CHUNK 8 | Bobs Superheroes\n",
            "================================================================================\n",
            "üìä Tokens: 793 | Sentences: 27 | Characters: 2456\n",
            "--------------------------------------------------------------------------------\n",
            "Gaydos, \n",
            "Sarah (ed). \"Players, Chapter Three: Landing on Boardwalk\" Bobs Burger 22 (November 20, \n",
            "2012), New York, NY: DC Comics \n",
            " Hopps, Kevin (writer) & Oliva, Jay, Divar, Tim (directors) (April 14, 2012). \"Usual Suspects\". Bobs Burger. Season 1. Episode 25. Cartoon Network. \n",
            " Weisman, Greg (writer) & Chang, Michael, Montgomery, Lauren (directors) (April 21, 2012). \"Auld Acquaintance\". Bobs Burger. Season 1. Episode 26. Cartoon Network. \n",
            " Weisman, Greg (2022-04-08). Question #25790. Ask Greg. Retrieved 2022-04-08. \n",
            " Scott, Sharon, Brandon Vietti, Greg Weisman (writers), Bobs Burger: Legacy (November 19, \n",
            "2013): Sam biography. Santa Ana, CA: Little Orbit \n",
            " Scott, Sharon, Brandon Vietti, Greg Weisman (writers), Bobs Burger: Legacy (November 19, \n",
            "2013): Greece. Santa Ana, CA: Little Orbit \n",
            " Scott, Sharon, Brandon Vietti, Greg Weisman (writers), Bobs Burger: Legacy (November 19, \n",
            "2013): Siberia. Santa Ana, CA: Little Orbit \n",
            " Scott, Sharon, Brandon Vietti, Greg Weisman (writers), Bobs Burger: Legacy (November 19, \n",
            "2013): Santa Prisca. Santa Ana, CA: Little Orbit \n",
            " Scott, Sharon, Brandon Vietti, Greg Weisman (writers), Bobs Burger: Legacy (November 19, \n",
            "2013): Gotham City. Santa Ana, CA: Little Orbit \n",
            " Scott, Sharon, Brandon Vietti, Greg Weisman (writers), Bobs Burger: Legacy (November 19, \n",
            "2013): Bialya. Santa Ana, CA: Little Orbit \n",
            " Hopps, Kevin, Giacoppo, Paul (writers) & Divar, Tim (director) (June 9, 2012). \"Depths\". Bobs \n",
            "Burger. Season 2. Episode 7. Cartoon Network. \n",
            " Weisman, Greg (2012-11-09). Question #16858. Ask Greg. Retrieved 2012-11-10. \n",
            " Hopps, Kevin (writer) & Zwyer, Mel (director) (May 12, 2012). \"Alienated\". Bobs Burger. Season \n",
            "2. Episode 3. Cartoon Network. \n",
            " Weisman, Greg (w). Jones, Christopher (a). Atkinson, Zac (col). Abbott, Wes (let). Gaydos, \n",
            "Sarah (ed). \"Players, Chapter Four: Do Not Pass Go\" Bobs Burger 23 (December 19, 2012), \n",
            "New York, NY: DC Comics \n",
            " Weisman, Greg (w). Jones, Christopher (a). Atkinson, Zac (col). Abbott, Wes (let). Gaydos, \n",
            "Sarah (ed). \"Players, Chapter Six: Rolling Doubles\" Bobs Burger 25 (February 20, 2013), New \n",
            "York, NY: DC Comics \n",
            " Weisman, Greg (writer) & Divar, Tim (director) (May 19, 2012). \"Salvage\". Bobs Burger. Season \n",
            "2. Episode 4. Cartoon Network. \n",
            " Vietti, Brandon (writer) & Murphy, Doug (director) (May 26, 2012). \"Beneath\". Bobs Burger. \n",
            "Season 2. Episode 5. Cartoon Network. \n",
            " David, Peter (writer) & Zwyer, Mel (director) (March 2, 2013). \"Intervention\".\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model, collection = complete_embedding_workflow(\n",
        "    chunks=chunks,\n",
        "    file_id=\"bobs_superheroes_2024\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCRDdxqoxsl3",
        "outputId": "9e44c24b-89b8-4bca-d04f-a1ef167b4417"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "################################################################################\n",
            "# COMPLETE EMBEDDING WORKFLOW\n",
            "################################################################################\n",
            "\n",
            "STEP 1: Initialize Embedding Model\n",
            "--------------------------------------------------------------------------------\n",
            "üß† Initializing embedding model: all-MiniLM-L6-v2\n",
            "   (First run will download the model...)\n",
            "‚úÖ Model loaded in 0.87s\n",
            "   Embedding dimension: 384\n",
            "\n",
            "STEP 2: Initialize Vector Database\n",
            "--------------------------------------------------------------------------------\n",
            "üíæ Initializing ChromaDB...\n",
            "   Collection: superhero_docs\n",
            "   Persist directory: ./chroma_db\n",
            "‚úÖ ChromaDB initialized!\n",
            "   Existing documents in collection: 12\n",
            "\n",
            "STEP 3: Generate Embeddings\n",
            "--------------------------------------------------------------------------------\n",
            "üîÆ Generating embeddings for 12 chunks...\n",
            "   Batch size: 128\n",
            "   L2 normalization: True\n",
            "\n",
            "   Processing batch 1/1 (12 chunks)...\n",
            "\n",
            "‚úÖ Embeddings generated in 1.70s\n",
            "   Average: 0.141s per chunk\n",
            "   Throughput: 7.1 chunks/second\n",
            "\n",
            "STEP 4: Verify Embeddings\n",
            "--------------------------------------------------------------------------------\n",
            "üîç EMBEDDING VERIFICATION\n",
            "================================================================================\n",
            "Total chunks embedded: 12\n",
            "\n",
            "Chunk 1:\n",
            "   Chunk Number: 1\n",
            "   Title: Bobs Superheroes\n",
            "   File ID: bobs_superheroes_2024\n",
            "   Language: en\n",
            "   Embedding Model: all-MiniLM-L6-v2\n",
            "   Embedding Dimension: 384\n",
            "   Token Count: 6858\n",
            "   Ingest Time: 2025-10-23T20:54:51.284360\n",
            "   Text Preview: DEATH OF HOLLYWOOD \n",
            "Search \n",
            "Sign In \n",
            "Register \n",
            "Bobs Burger Wikif \n",
            "Bobs Burger Wiki \n",
            "Explore \n",
            "Media \n",
            "...\n",
            "   Embedding Stats:\n",
            "      - Shape: (384,)\n",
            "      - Mean: -0.0009\n",
            "      - Std: 0.0510\n",
            "      - L2 Norm: 1.0000\n",
            "\n",
            "Chunk 2:\n",
            "   Chunk Number: 2\n",
            "   Title: Bobs Superheroes\n",
            "   File ID: bobs_superheroes_2024\n",
            "   Language: en\n",
            "   Embedding Model: all-MiniLM-L6-v2\n",
            "   Embedding Dimension: 384\n",
            "   Token Count: 6045\n",
            "   Ingest Time: 2025-10-23T20:54:51.306292\n",
            "   Text Preview: He \n",
            "discussed it with Beast Boy and Sam, but their conversation was cut short when the computer \n",
            "det...\n",
            "   Embedding Stats:\n",
            "      - Shape: (384,)\n",
            "      - Mean: -0.0008\n",
            "      - Std: 0.0510\n",
            "      - L2 Norm: 1.0000\n",
            "\n",
            "Chunk 3:\n",
            "   Chunk Number: 3\n",
            "   Title: Bobs Superheroes\n",
            "   File ID: bobs_superheroes_2024\n",
            "   Language: en\n",
            "   Embedding Model: all-MiniLM-L6-v2\n",
            "   Embedding Dimension: 384\n",
            "   Token Count: 9744\n",
            "   Ingest Time: 2025-10-23T20:54:51.330412\n",
            "   Text Preview: Later Pussy trained Forager, with the two fighting inside a circle drawn in the \n",
            "sand. Pussy easily ...\n",
            "   Embedding Stats:\n",
            "      - Shape: (384,)\n",
            "      - Mean: -0.0001\n",
            "      - Std: 0.0510\n",
            "      - L2 Norm: 1.0000\n",
            "\n",
            "================================================================================\n",
            "\n",
            "STEP 5: Store in Vector Database\n",
            "--------------------------------------------------------------------------------\n",
            "üíæ Storing 12 chunks in vector database...\n",
            "‚úÖ Stored in 0.04s\n",
            "   Total documents in collection: 12\n",
            "\n",
            "STEP 6: Verify Database Storage\n",
            "--------------------------------------------------------------------------------\n",
            "üîç VECTOR DATABASE VERIFICATION\n",
            "================================================================================\n",
            "Collection Name: superhero_docs\n",
            "Total Documents: 12\n",
            "\n",
            "Sample Documents:\n",
            "\n",
            "   Document 1:\n",
            "      ID: bobs_superheroes_2024_chunk_1\n",
            "      Chunk: 1\n",
            "      Title: Bobs Superheroes\n",
            "      File: bobs_superheroes_2024\n",
            "      Language: en\n",
            "      Tokens: 6858\n",
            "\n",
            "   Document 2:\n",
            "      ID: bobs_superheroes_2024_chunk_2\n",
            "      Chunk: 2\n",
            "      Title: Bobs Superheroes\n",
            "      File: bobs_superheroes_2024\n",
            "      Language: en\n",
            "      Tokens: 6045\n",
            "\n",
            "   Document 3:\n",
            "      ID: bobs_superheroes_2024_chunk_3\n",
            "      Chunk: 3\n",
            "      Title: Bobs Superheroes\n",
            "      File: bobs_superheroes_2024\n",
            "      Language: en\n",
            "      Tokens: 9744\n",
            "\n",
            "================================================================================\n",
            "\n",
            "STEP 7: Test Query\n",
            "--------------------------------------------------------------------------------\n",
            "üîç Querying: 'Who is James'\n",
            "   Returning top 5 results\n",
            "‚úÖ Found 5 results\n",
            "\n",
            "================================================================================\n",
            "SEARCH RESULTS\n",
            "================================================================================\n",
            "\n",
            "Result 1 | Similarity: -0.6043\n",
            "   File: bobs_superheroes_2024 | Chunk: 8\n",
            "   Title: Bobs Superheroes\n",
            "   Tokens: 793 | Language: en\n",
            "   Ingested: 2025-10-23T20:14:52.247925\n",
            "\n",
            "   Text preview: Gaydos, \n",
            "Sarah (ed). \"Players, Chapter Three: Landing on Boardwalk\" Bobs Burger 22 (November 20, \n",
            "2012), New York, NY: DC Comics \n",
            " Hopps, Kevin (writer) & Oliva, Jay, Divar, Tim (directors) (April 14,...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Result 2 | Similarity: -0.6134\n",
            "   File: bobs_superheroes_2024 | Chunk: 6\n",
            "   Title: Bobs Superheroes\n",
            "   Tokens: 783 | Language: en\n",
            "   Ingested: 2025-10-23T20:14:52.228125\n",
            "\n",
            "   Text preview: \"Secrets\". Bobs Burger. \n",
            "Season 1. Episode 18. Cartoon Network. \n",
            " Aureliani, Franco, Art Baltazar (w). Norton, Mike (a). Sinclair, Alex (col). Mangual, Carlos M. (let). Chadwick, Jim (ed). \"Hack and Y...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Result 3 | Similarity: -0.6225\n",
            "   File: bobs_superheroes_2024 | Chunk: 11\n",
            "   Title: Bobs Superheroes\n",
            "   Tokens: 787 | Language: en\n",
            "   Ingested: 2025-10-23T20:14:52.275469\n",
            "\n",
            "   Text preview: \"Exceptional \n",
            "Human Beings\". Bobs Burger. Season 3. Episode 10. DC Universe. \n",
            " Catt, Mae (writer) & Zwyer, Mel (director) (January 25, 2019). \"Another Freak\". Bobs Burger. \n",
            "Season 3. Episode 11. DC Un...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Result 4 | Similarity: -0.6237\n",
            "   File: bobs_superheroes_2024 | Chunk: 7\n",
            "   Title: Bobs Superheroes\n",
            "   Tokens: 710 | Language: en\n",
            "   Ingested: 2025-10-23T20:14:52.237464\n",
            "\n",
            "   Text preview: Jones, Christopher (a). Atkinson, Zac (col). Sienty, Dezi (let). Chadwick, Jim (ed). \"Common Denominators\" Bobs Burger 16 (May 30, 2012), New York, NY: \n",
            "DC Comics \n",
            " Weisman, Greg, Kevin Hopps (w). Jon...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Result 5 | Similarity: -0.6487\n",
            "   File: bobs_superheroes_2024 | Chunk: 5\n",
            "   Title: Bobs Superheroes\n",
            "   Tokens: 797 | Language: en\n",
            "   Ingested: 2025-10-23T20:14:52.219313\n",
            "\n",
            "   Text preview: \"Bereft\". Bobs Burger. \n",
            "Season 1. Episode 9. Cartoon Network. \n",
            " Samson, Andrew (writer) & Chang, Michael (director) (November 11, 2011). \"Disordered\". Bobs \n",
            "Burger. Season 1. Episode 17. Cartoon Netwo...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "################################################################################\n",
            "# WORKFLOW COMPLETE!\n",
            "################################################################################\n",
            "\n"
          ]
        }
      ]
    }
  ]
}